import requests
from io import BytesIO
from PIL import Image
import uvicorn
import numpy as np
import faiss

from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from linebot import LineBotApi, WebhookHandler
from linebot.models import (
    MessageEvent,
    TextMessage,
    ImageMessage,
    FlexSendMessage,
    BubbleContainer,
    BoxComponent,
    TextComponent,
    ButtonComponent,
    URIAction,
    CarouselContainer
)
from linebot.exceptions import InvalidSignatureError
from sentence_transformers import SentenceTransformer
from typing import Dict
from contextlib import asynccontextmanager

# ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ dotenv ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å .env
# from dotenv import load_dotenv  

app = FastAPI()

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ os.getenv
ACCESS_TOKEN = "RMuXBCLD7tGSbkGgdELH7Vz9+Qz0YhqCIeKBhpMdKvOVii7W2L9rNpAHjYGigFN4ORLknMxhuWJYKIX3uLrY1BUg7E3Bk0v3Fmc5ZIC53d8fOdvIMyZQ6EdaOS0a6kejeqcX/dRFI/JfiFJr5mdwZgdB04t89/1O/w1cDnyilFU="
CHANNEL_SECRET = "175149695b4d312eabb9df4b7e3e7a95"
GEMINI_API_KEY = "AIzaSyBfkFZ8DCBb57CwW8WIwqSbUTB3fyIfw6g"

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
if not ACCESS_TOKEN or not CHANNEL_SECRET or not GEMINI_API_KEY:
    raise ValueError("Please set the LINE_ACCESS_TOKEN, LINE_CHANNEL_SECRET, and GEMINI_API_KEY environment variables.")

line_bot_api = LineBotApi(ACCESS_TOKEN)
handler = WebhookHandler(CHANNEL_SECRET)

class GeminiRAGSystem:
    def __init__(self, 
                 json_db_path: str, 
                 gemini_api_key: str, 
                 embedding_model: str = 'all-MiniLM-L6-v2'):
        # ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Gemini
        genai.configure(api_key=gemini_api_key)
        
        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        self.generation_model = genai.GenerativeModel('gemini-1.5-flash')
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.json_db_path = json_db_path
        
        # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # Load ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON
        self.load_database()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index
        self.create_faiss_index()
    
    def load_database(self):
        """Load existing database or create new"""
        try:
            with open(self.json_db_path, 'r', encoding='utf-8') as f:
                self.database = json.load(f)
        except FileNotFoundError:
            self.database = {
                'documents': [],
                'embeddings': [],
                'metadata': []
            }
    
    def save_database(self):
        """Save database to JSON file"""
        with open(self.json_db_path, 'w', encoding='utf-8') as f:
            json.dump(self.database, f, indent=2, ensure_ascii=False)
     
    def add_document(self, text: str, metadata: dict = None):
        """Add document to database with embedding"""
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        embedding = self.embedding_model.encode([text])[0]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.database['documents'].append(text)
        self.database['embeddings'].append(embedding.tolist())
        self.database['metadata'].append(metadata or {})
        
        # Save ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON
        self.save_database()
        self.create_faiss_index()
    
    def create_faiss_index(self):
        """Create FAISS index for similarity search"""
        if not self.database['embeddings']:
            self.index = None
            return
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô numpy array
        embeddings = np.array(self.database['embeddings'], dtype='float32')
        dimension = embeddings.shape[1]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index
        self.index = faiss.IndexFlatL2(dimension)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô FAISS index
        self.index.add(embeddings)
    
    def retrieve_documents(self, query: str, top_k: int = 3):
        """Retrieve most relevant documents"""
        if not self.database['embeddings'] or self.index is None:
            return []
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
        query_embedding = self.embedding_model.encode([query]).astype('float32')
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ similarity search
        D, I = self.index.search(query_embedding, top_k)
        
        return [self.database['documents'][i] for i in I[0] if i < len(self.database['documents'])]
    
    def generate_response(self, query: str):
        """Generate response using Gemini and retrieved documents"""
        # Retrieve ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        retrieved_docs = self.retrieve_documents(query)
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
        context = "\n\n".join(retrieved_docs)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        full_prompt = f"""You are an AI assistant. 
Use the following context to answer the question precisely:

Context:
{context}

Question: {query}

Provide a detailed and informative response based on the context in Thai 
but if the response is not about the context just ignore and answer in a natural way."""
        
        # ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Gemini
        try:
            response = self.generation_model.generate_content(full_prompt)
            return response.text, full_prompt
        except Exception as e:
            return f"Error generating response: {str(e)}", str(e)
    
    def generate_concise_response(self, query: str):
        """Generate a concise response using Gemini without RAG context"""
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å RAG
        concise_prompt = f"""You are an AI assistant. 
Provide a concise and to-the-point answer to the following question in Thai:

Question: {query}"""
        
        try:
            response = self.generation_model.generate_content(
                concise_prompt,
                generation_config={
                    "max_output_tokens": 100,  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    "temperature": 0.5,
                    "top_p": 0.9,
                    "top_k": 40
                }
            )
            return response.text
        except Exception as e:
            return f"Error generating response: {str(e)}"
    
    def process_image_query(self, 
                            image_content: bytes, 
                            query: str,
                            use_rag: bool = True,
                            top_k_docs: int = 3) -> Dict:
        """
        Process image-based query with optional RAG enhancement
        
        Args:
            image_content (bytes): Content of the image
            query (str): Query about the image
            use_rag (bool): Whether to use RAG for context
            top_k_docs (int): Number of documents to retrieve
        
        Returns:
            Generated response about the image
        """
        # ‡πÄ‡∏õ‡∏¥‡∏î‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤
        image = Image.open(BytesIO(image_content))

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û
        initial_description = self.generation_model.generate_content(
            ["Provide a detailed, objective description of this image", image],
            generation_config={
                "max_output_tokens": 256,
                "temperature": 0.4,
                "top_p": 0.9,
                "top_k": 8
            }
        ).text
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô RAG 
        context = ""
        if use_rag:
            # ‡∏ô‡∏≥‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô JSON
            retrieved_docs = self.retrieve_documents(initial_description, top_k_docs)
            
            # ‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
            context = "\n\n".join(retrieved_docs)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        enhanced_prompt = f"""Image Description:
{initial_description}

Context from Knowledge Base:
{context}

User Query: {query}

Based on the image description and the contextual information from our knowledge base, 
provide a comprehensive and insightful response to the query. 
If the context does not directly relate to the image, focus on the image description 
and your visual analysis in Thai."""
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Gemini
        try:
            response = self.generation_model.generate_content(
                [enhanced_prompt, image],
                generation_config={
                    "max_output_tokens": 256,
                    "temperature": 0.4,
                    "top_p": 0.9,
                    "top_k": 8
                }
            )
            
            return {
                "final_response": response.text,
            }
        except Exception as e:
            return {
                "error": f"Error generating response: {str(e)}",
                "image_description": initial_description
            }
    
    def clear_database(self):
        """Clear database and save to JSON file"""
        self.database = {
            'documents': [],
            'embeddings': [],
            'metadata': []
        }
        self.save_database()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Object ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Gemini
gemini = GeminiRAGSystem(
    json_db_path="gemini_rag_database.json", 
    gemini_api_key=GEMINI_API_KEY
)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gemini
    sample_documents = [
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    ]
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏á‡πÉ‡∏ô Gemini
    for doc in sample_documents:
        gemini.add_document(doc)
        
    yield

    # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Gemini
    gemini.clear_database()

app = FastAPI(lifespan=lifespan)

# Manuals
EMERGENCY_MANUAL = """
üì± ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô WILDSAFE ü¶Æ

üî∏ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô
   ‚Ä¢ ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "Emergency" ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠
   ‚Ä¢ ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô
"""

WATCH_ELEPHANT_MANUAL = """
üêò ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏ä‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏Å‡∏•‡πâ üö®

1. üòå ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏ï‡∏¥ - ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÉ‡∏´‡πâ‡∏™‡∏á‡∏ö ‡πÑ‡∏°‡πà‡∏ï‡∏∑‡πà‡∏ô‡∏ï‡∏£‡∏∞‡∏´‡∏ô‡∏Å
2. üëÄ ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ö‡∏ï‡∏≤ - ‡∏≠‡∏¢‡πà‡∏≤‡∏à‡πâ‡∏≠‡∏á‡∏°‡∏≠‡∏á‡∏ä‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
3. üö∂‚Äç‚ôÇÔ∏è ‡∏ñ‡∏≠‡∏¢‡∏≠‡∏≠‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡πâ‡∏≤‡πÜ - ‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
"""

CHECK_ELEPHANT_MANUAL = """
üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á!

üêò ‡πÄ‡∏ä‡πá‡∏Ñ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ä‡πâ‡∏≤‡∏á‡∏õ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
üëâ ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà: https://aprlabtop.com/Honey_test/chang_v3.php
"""

OFFICER_MANUAL = """
üìû ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Ç‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠ üÜò

üöë ‡πÄ‡∏´‡∏ï‡∏∏‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô 24 ‡∏ä‡∏°.: 1669
üèïÔ∏è ‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏Å‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß: 086-092-6529
"""

def get_manual_response(user_message: str) -> str:
    user_message = user_message.strip().lower()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    if user_message in ["emergency", "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"]:
        return EMERGENCY_MANUAL
    elif user_message in ["emergency ‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á", "‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏£‡πâ‡∏≤‡∏¢‡πÉ‡∏Å‡∏•‡πâ‡∏ï‡∏±‡∏ß"]:
        return WATCH_ELEPHANT_MANUAL
    elif user_message == "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡πâ‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á":
        return CHECK_ELEPHANT_MANUAL
    elif user_message in ["‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà", "contact officer"]:
        return OFFICER_MANUAL
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á
    elephant_keywords = ['‡∏ä‡πâ‡∏≤‡∏á', '‡∏û‡∏ö‡∏ä‡πâ‡∏≤‡∏á', '‡πÄ‡∏à‡∏≠‡∏ä‡πâ‡∏≤‡∏á', '‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏ä‡πâ‡∏≤‡∏á']
    if any(keyword in user_message for keyword in elephant_keywords):
        return WATCH_ELEPHANT_MANUAL
    
    return None

def create_bubble_container(text: str) -> BubbleContainer:
    return BubbleContainer(
        header=BoxComponent(
            layout='vertical',
            contents=[
                TextComponent(
                    text="WildSafe",
                    weight='bold',
                    align='center',
                    color='#FFFFFF',
                    size='xl'
                )
            ],
            background_color='#27AE60'
        ),
        body=BoxComponent(
            layout='vertical',
            contents=[
                TextComponent(
                    text=text,
                    wrap=True,
                    size='sm'
                )
            ]
        ),
        footer=BoxComponent(
            layout='vertical',
            contents=[
                ButtonComponent(
                    style='primary',
                    action=URIAction(
                        label='GO MAP',
                        uri='https://aprlabtop.com/Honey_test/chang_v3.php'
                    )
                )
            ]
        )
    )

def create_flex_message(text: str) -> FlexSendMessage:
    bubble = create_bubble_container(text)
    return FlexSendMessage(alt_text="WildSafe Message", contents=bubble)

def create_carousel_message(texts: list) -> FlexSendMessage:
    bubbles = [create_bubble_container(text) for text in texts]
    carousel = CarouselContainer(contents=bubbles)
    return FlexSendMessage(alt_text="WildSafe Carousel", contents=carousel)

@app.post('/message')
async def message(request: Request):
    signature = request.headers.get('X-Line-Signature')
    if not signature:
        raise HTTPException(status_code=400, detail="X-Line-Signature header is missing")
    
    body = await request.body()
    try:
        handler.handle(body.decode("UTF-8"), signature)
    except InvalidSignatureError:
        raise HTTPException(status_code=400, detail="Invalid signature")

@handler.add(MessageEvent, message=(TextMessage, ImageMessage))
def handle_message(event: MessageEvent):
    if isinstance(event.message, TextMessage):
        user_message = event.message.text
        manual_response = get_manual_response(user_message)
        
        if manual_response:
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å manual
            reply = create_flex_message(manual_response)
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Gemini ‡∏ï‡∏≠‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
            gemini_response = gemini.generate_concise_response(user_message)
            
            if "Error" not in gemini_response:
                reply = create_flex_message(gemini_response)
            else:
                # ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Gemini ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                retrieved_docs = gemini.retrieve_documents(user_message, top_k=3)
                
                if retrieved_docs:
                    texts = ["‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà" if "http" in doc else doc for doc in retrieved_docs]
                    reply = create_carousel_message(texts)
                else:
                    reply = create_flex_message("‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")

    elif isinstance(event.message, ImageMessage):
        try:
            headers = {"Authorization": f"Bearer {ACCESS_TOKEN}"}
            url = f"https://api-data.line.me/v2/bot/message/{event.message.id}/content"
            response = requests.get(url, headers=headers, stream=True)
            response.raise_for_status()
            image_data = BytesIO(response.content)
            image = Image.open(image_data)
            
            if image.size[0] * image.size[1] > 1024 * 1024:
                message = "‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö ‡∏†‡∏≤‡∏û‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
            else:
                # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û‡πÑ‡∏õ‡∏¢‡∏±‡∏á Gemini ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
                gemini_response = gemini.process_image_query(
                    image_content=response.content,
                    query="‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", 
                    use_rag=True
                )
                # ‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Gemini ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
                response_text = gemini_response.get('final_response', "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏â‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ")
                message = response_text
                
        except Exception:
            message = "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î, ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏áüôèüèª"
            
        reply = create_flex_message(message)
    
    line_bot_api.reply_message(
        event.reply_token,
        [reply]
    )

@app.get('/test-message')
async def test_message_gemini(text: str):
    """
    Debug message from Gemini
    """
    response, prompt = gemini.generate_response(text)

    return {
        "gemini_answer": response,
        "full_prompt": prompt
    }

@app.post('/image-query')
async def image_query(
    file: UploadFile = File(...), 
    query: str = Form("‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"),
    use_rag: bool = Form(True)
):
    if file.size > 1024 * 1024:
        raise HTTPException(status_code=400, detail="Image size too large")
    
    contents = await file.read()
    
    # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û‡πÑ‡∏õ‡∏¢‡∏±‡∏á Gemini ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    image_response = gemini.process_image_query(
        image_content=contents,
        query=query,
        use_rag=use_rag
    )
    
    return image_response

if __name__ == "__main__":
    uvicorn.run("main:app", port=8000, host="0.0.0.0")
